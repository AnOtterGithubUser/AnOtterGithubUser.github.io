<!DOCTYPE HTML>
<!--
	Introspect by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Linear regression - Yet another data science blog...</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript"
		  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
		<script
		    src="https://code.jquery.com/jquery-3.3.1.js"
		    integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
		    crossorigin="anonymous">
		</script>
		<script>
		$(function(){
		  $("#header").load("header.html");
		  $("#footer").load("footer.html");
		  $("#content").load("linear_regression_content.html");
		});
		</script>
	</head>
	<body>

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="index.html"><span class="logo" style="font-size: 2.3em">Yet another data science blog...</span></a>
					<nav id="nav">
						<a href="index.html">Home</a>
						<a href="blog.html">Blog</a>
						<a href="about.html">About</a>
					</nav>
				</div>
			</header>
			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

		<!-- Main -->
		<section id="main" >
			<div class="inner">
				<header class="major special">
					<center><h1>Linear regression</h1><p>September 5th, 2018</p></center>
				</header>
				In this tutorial we will suppose that we have a matrix of samples $X$ and the corresponding labels $Y$
				and we want to find the coefficients $w$ such that $Y = w^T X$.<br>
				Basically, we want to characterize a function $f$ such that:
				\[f: X \rightarrow w_0 + \sum_{j=1}^p X_j w_j \]
				Of course, if the samples $X$ were not drawn from a linear distribution, we will not be able to find such function. This is a simple model and the data
				are rarely linearly separable. However it is still useful since it is a simple expressive model that requires few parameters. We need to find the hyperplan that fits the data as accurately as possible. Let's suppose we have a dataset of $((x_i, y_i))_{i \in \{1..N\}}$<br>
				Then we will need a loss function to tell us how good our solution is. The most popular method is the least squares where the loss function is the
				residual sum of squares:
				\[L: X,Y,w \rightarrow \sum_{i=1}^N (y_i- f(x_i))^2 \]
				The problem is then:
				\[ min_{w} L(X, Y, w) \]
				Actually, we can write $L(X,Y,w)$ in matricial form: $L(X,Y,w) = (y - X w)^T (y - X w)$. We have a quadratic problem which we can differenciate
				with respect to $w$:
				\[ \frac{\partial L(X,Y,w)}{\partial w} = -2 X^T (y - Xw) \]
				\[ \frac{\partial^2 L(X,Y,w)}{\partial w \partial w^T} = 2 X^T X \]
				If $X$ has full rank, we can set the first derivative to $0$ and we obtain the unique solution:
				\[\hat{w} = (X^T X)^{-1} X^T y \]
				Then our function $f$ is:
				\[ f: X \rightarrow X (X^T X)^{-1} X^T y \]
				Now if $X$ does not have full rank, $X^T X$ is singular and the solution is not unique.<br>
				On a geometric point of view, linear regression is a projection on the space spanned by the columns of $X$. In the case where $X$ does not have full rank, there are some redundancies between its columns. A usual way to solve this problem is to drop the redundant columns and get back to the full
				rank case. We may also use the Moore-Penrose pseudo inverse in practice when we implement this solution.
			</div>
		</section>
		<p class="signature">Thomas PESNEAU</p>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>